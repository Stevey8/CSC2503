{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc8f397",
   "metadata": {},
   "source": [
    "# Fruit Freshness Classification\n",
    "Important Notes (Please Read Before Using This Notebook):\n",
    "\n",
    "- This notebook was originally developed and tested in Google Colab using files stored in a personal Google Drive.\n",
    "\n",
    "- The version published on GitHub is provided for code reference only.\n",
    "\n",
    "- Running this notebook as-is will not work, because:\n",
    "\n",
    "    - Dataset files are not included in the repository (due to size and privacy).\n",
    "\n",
    "    - All file paths point to my personal Drive directory, which other users will not have access to.\n",
    "\n",
    "    - The notebook pushed from VS Code contains no runtime outputs, because it is not executed before uploading.\n",
    "\n",
    "If you wish to run this notebook yourself, you will need to:\n",
    "\n",
    "1. Provide your own dataset following the folder structure described in the report.\n",
    "\n",
    "2. Update all paths to match your environment (local, Colab, or custom).\n",
    "\n",
    "3. Mount your own Google Drive (if using Colab) or adjust file loading accordingly.\n",
    "\n",
    "All results, charts, metrics, comparisons, and discussion can be found in our project report and presentation, which serve as the authoritative sources for model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install for google colab\n",
    "!pip install -q ultralytics\n",
    "!pip install timm -q\n",
    "!pip install opencv-python pillow-heif ultralytics --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, zipfile, math, random, copy, shutil, time, datetime, csv, gc, json, glob, pillow_heif\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch, Rectangle\n",
    "%matplotlib inline\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from tqdm import tqdm, trange\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# torch and nn \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset, random_split\n",
    "from torchvision import transforms, utils, models, datasets\n",
    "import torch.nn as nn\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import timm\n",
    "\n",
    "import cv2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30249f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if using google colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d8c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fruit classes for reference \n",
    "fruit_classes = {\n",
    "    'kaggle': ('apples','bananas','oranges'),  # 3 types\n",
    "    'custom': ('avocado','grapes','lemon','mango','pineapple','strawberry','watermelon'), # 7 types\n",
    "    'ood': ('peach','tomato') # out of distribution (won't be seen by model)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f67d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "# modify if needed \n",
    "dataset_root = \"/content/project/fruits_binary\"\n",
    "test_root = os.path.join(dataset_root, \"test\")\n",
    "ood_test_root = '/content/project/ood_test'\n",
    "base_logs_root = Path(\"/content/drive/MyDrive/CSC2503\")\n",
    "\n",
    "# model logs paths\n",
    "# example: yolo_logs_root = Path(\"/content/drive/MyDrive/CSC2503/yolo_training_logs\")\n",
    "yolo_logs_root = base_logs_root / \"yolo_training_logs\"\n",
    "swin_logs_root = base_logs_root / \"swin_training_logs\"\n",
    "repvgg_logs_root = base_logs_root / \"repvgg_training_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd01530",
   "metadata": {},
   "source": [
    "# Training \n",
    "**(Skip this section if models have been trained. Proceed to next section where models/pre-trained weights will be loaded)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify path as needed\n",
    "\n",
    "# unzip the complete dataset\n",
    "!unzip /content/drive/MyDrive/CSC2503/fruits_binary.zip -d /content/project\n",
    "# unzip the ood test set\n",
    "!unzip /content/drive/MyDrive/CSC2503/ood_test.zip -d /content/project\n",
    "\n",
    "# useful bash commands for checking files/dirs\n",
    "# !ls /content/drive/MyDrive/CSC2503\n",
    "# !ls -F /content/project/fruits_binary\n",
    "# !ls -F /content/project/ood_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4390ddd3",
   "metadata": {},
   "source": [
    "### YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acd4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_logs_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# set this True if you want to force retrain even if logs exist\n",
    "force_retrain = False  \n",
    "\n",
    "# --- 1) Try to load an existing best.pt from yolo_logs_root/weights/best.pt ---\n",
    "best_candidates = sorted(yolo_logs_root.glob(\"weights/best.pt\"))\n",
    "\n",
    "if best_candidates and not force_retrain:\n",
    "    # pick the latest run (last in sorted list)\n",
    "    best_weight = best_candidates[-1]\n",
    "    print(f\"Found existing trained model: {best_weight}\")\n",
    "    print(\"Loading model without retraining...\")\n",
    "    yolo_model = YOLO(str(best_weight))\n",
    "    print(\"model loaded! call 'yolo_model' to access the model.\")\n",
    "\n",
    "else:\n",
    "    print(\"No existing model found in logs OR retrain requested.\")\n",
    "    print(\"Training a new YOLO classification model...\")\n",
    "\n",
    "    # 2) Train from pretrained YOLO11m classification weights\n",
    "    yolo_model = YOLO(\"yolo11m-cls.pt\")  # or \"yolo11s-cls.pt\", etc.\n",
    "\n",
    "    results = yolo_model.train(\n",
    "        data=dataset_root,      # root with train/val(/test)\n",
    "        epochs=15,\n",
    "        imgsz=224,\n",
    "        batch=16,\n",
    "        device=0,               # GPU in Colab\n",
    "        # classes=2,            # YOLO infers from folders ['bad', 'good']\n",
    "        auto_augment=\"randaugment\",\n",
    "        fliplr=0.5,\n",
    "        degrees=15,\n",
    "        scale=0.5,\n",
    "        freeze=range(1, 9999),  # freeze all but first layer + head\n",
    "    )\n",
    "    print(\"YOLO model trained; you can access the model by calling 'yolo_model'.\")\n",
    "\n",
    "    # 3) Mirror YOLO runs from /content/runs/classify/train* → Drive logs\n",
    "    runs_root = Path(\"runs\") / \"classify\"   # usually /content/runs/classify\n",
    "    if runs_root.exists():\n",
    "        train_runs = list(runs_root.glob(\"train*\"))\n",
    "        if train_runs:\n",
    "            for run in train_runs:\n",
    "                dest = yolo_logs_root / run.name\n",
    "                print(f\"Copying {run} → {dest}\")\n",
    "                shutil.copytree(run, dest, dirs_exist_ok=True)\n",
    "        else:\n",
    "            print(\"WARNING: no train* runs found under runs/classify.\")\n",
    "    else:\n",
    "        print(\"WARNING: runs/classify folder not found; nothing copied to yolo_logs_root.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a767b",
   "metadata": {},
   "source": [
    "### RepVGG and Swin Transformer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5764680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets & loaders\n",
    "img_size = 224\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dir = os.path.join(dataset_root, \"train\")\n",
    "val_dir   = os.path.join(dataset_root, \"val\")\n",
    "\n",
    "train_ds = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "val_ds   = datasets.ImageFolder(val_dir,   transform=val_transform)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"Classes:\", train_ds.classes, \"num_classes:\", len(train_ds.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c227e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "# helpers\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk_req = max(topk)\n",
    "        num_classes = output.size(1)\n",
    "        maxk = min(maxk_req, num_classes)  # don't ask for more than num_classes\n",
    "\n",
    "        batch_size = target.size(0)\n",
    "        _, pred = output.topk(maxk, dim=1, largest=True, sorted=True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            k_eff = min(k, num_classes)  # clamp each k as well\n",
    "            correct_k = correct[:k_eff].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append((correct_k / batch_size).item())\n",
    "        return res\n",
    "\n",
    "# train / eval loops\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc1 = 0.0\n",
    "    running_acc5 = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "\n",
    "        running_loss += loss.item() * batch_size\n",
    "        running_acc1 += acc1 * batch_size\n",
    "        running_acc5 += acc5 * batch_size\n",
    "        n_samples += batch_size\n",
    "\n",
    "    epoch_loss = running_loss / n_samples\n",
    "    epoch_acc1 = running_acc1 / n_samples\n",
    "    epoch_acc5 = running_acc5 / n_samples\n",
    "    return epoch_loss, epoch_acc1, epoch_acc5\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_acc1 = 0.0\n",
    "    running_acc5 = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            batch_size = labels.size(0)\n",
    "            acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "\n",
    "            running_loss += loss.item() * batch_size\n",
    "            running_acc1 += acc1 * batch_size\n",
    "            running_acc5 += acc5 * batch_size\n",
    "            n_samples += batch_size\n",
    "\n",
    "    epoch_loss = running_loss / n_samples\n",
    "    epoch_acc1 = running_acc1 / n_samples\n",
    "    epoch_acc5 = running_acc5 / n_samples\n",
    "    return epoch_loss, epoch_acc1, epoch_acc5\n",
    "\n",
    "\n",
    "# model factory functions\n",
    "def create_swin_model(num_classes):\n",
    "    swin = models.swin_t(weights=models.Swin_T_Weights.IMAGENET1K_V1)\n",
    "    # replace head for binary classification\n",
    "    swin.head = nn.Linear(swin.head.in_features, num_classes)\n",
    "    return swin\n",
    "\n",
    "\n",
    "def create_repvgg_model(num_classes):\n",
    "    # RepVGG via timm\n",
    "    model = timm.create_model(\"repvgg_b0\", pretrained=True, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "# generic train-or-load wrapper\n",
    "def train_or_load_model(model_name, create_model_fn, num_epochs=15, lr=1e-4, wd=1e-4, force_retrain=False):\n",
    "    \"\"\"\n",
    "    model_name: \"swin\" or \"repvgg\"\n",
    "    create_model_fn: function that returns a fresh model instance\n",
    "    Logs + weights go to: /content/drive/MyDrive/CSC2503/{model_name}_training_logs/\n",
    "    \"\"\"\n",
    "    logs_root = base_logs_root / f\"{model_name}_training_logs\"\n",
    "    logs_root.mkdir(parents=True, exist_ok=True)\n",
    "    weight_path  = logs_root / \"best.pth\"\n",
    "    metrics_path = logs_root / \"metrics.json\"\n",
    "\n",
    "    # If weights already exist and we don't want to retrain, just load and return model\n",
    "    if weight_path.exists() and not force_retrain:\n",
    "        print(f\"[{model_name}] Found existing weights at {weight_path}. Loading model...\")\n",
    "        model = create_model_fn(num_classes)\n",
    "        state = torch.load(weight_path, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        # Optionally load metrics as well\n",
    "        if metrics_path.exists():\n",
    "            with open(metrics_path, \"r\") as f:\n",
    "                history = json.load(f)\n",
    "            print(f\"[{model_name}] Loaded existing metrics from {metrics_path}.\")\n",
    "        else:\n",
    "            history = None\n",
    "        return model, history\n",
    "\n",
    "    # Otherwise: train from scratch / pretrained backbone\n",
    "    print(f\"[{model_name}] No existing weights found or retrain forced. Training for {num_epochs} epochs...\")\n",
    "    model = create_model_fn(num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    best_val_acc1 = 0.0\n",
    "    history = {\n",
    "        \"epoch\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc1\": [],\n",
    "        \"train_acc5\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc1\": [],\n",
    "        \"val_acc5\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(1, num_epochs + 1), desc=f\"{model_name} epochs\"):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc1, train_acc5 = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc1, val_acc5       = evaluate(model, val_loader, criterion, device)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(\n",
    "            f\"[{model_name}] Epoch {epoch:02d}/{num_epochs} \"\n",
    "            f\"- {elapsed:.1f}s \"\n",
    "            f\"- train_loss: {train_loss:.4f}, train_acc1: {train_acc1:.4f}, train_acc5: {train_acc5:.4f} \"\n",
    "            f\"- val_loss: {val_loss:.4f}, val_acc1: {val_acc1:.4f}, val_acc5: {val_acc5:.4f}\"\n",
    "        )\n",
    "\n",
    "        # log history\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc1\"].append(train_acc1)\n",
    "        history[\"train_acc5\"].append(train_acc5)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc1\"].append(val_acc1)\n",
    "        history[\"val_acc5\"].append(val_acc5)\n",
    "\n",
    "        # save best weights\n",
    "        if val_acc1 > best_val_acc1:\n",
    "            best_val_acc1 = val_acc1\n",
    "            torch.save(model.state_dict(), weight_path)\n",
    "            print(f\"[{model_name}] New best val_acc1: {best_val_acc1:.4f}. Saved weights to {weight_path}\")\n",
    "\n",
    "    # save metrics to JSON for later analysis\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(f\"[{model_name}] Training complete. Metrics saved to {metrics_path}\")\n",
    "\n",
    "    # load best weights back into model (in case last epoch wasn't best)\n",
    "    state = torch.load(weight_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a3597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train or load SWIN\n",
    "swin_model, swin_history = train_or_load_model(\n",
    "    model_name=\"swin\",\n",
    "    create_model_fn=create_swin_model,\n",
    "    num_epochs=15,\n",
    "    lr=1e-4,\n",
    "    wd=1e-4,\n",
    "    force_retrain=False,    # set True if you want to retrain\n",
    ")\n",
    "\n",
    "# Train or load RepVGG\n",
    "repvgg_model, repvgg_history = train_or_load_model(\n",
    "    model_name=\"repvgg\",\n",
    "    create_model_fn=create_repvgg_model,\n",
    "    num_epochs=15,\n",
    "    lr=1e-4,\n",
    "    wd=1e-4,\n",
    "    force_retrain=False,    # set True if you want to retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b314bd",
   "metadata": {},
   "source": [
    "# Load the trained model\n",
    "**(RUN THIS SECTION)**\n",
    "- Access the models by calling \n",
    "    - yolo_model\n",
    "    - repvgg_model\n",
    "    - swin_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbdb345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo \n",
    "best_candidates = sorted(yolo_logs_root.glob(\"weights/best.pt\"))\n",
    "if best_candidates:\n",
    "    # pick the latest run (last in sorted list)\n",
    "    best_weight = best_candidates[-1]\n",
    "    print(f\"Found existing trained model: {best_weight}\")\n",
    "    print(\"Loading model without retraining...\")\n",
    "    yolo_model = YOLO(str(best_weight))\n",
    "    print(\"model loaded! call 'yolo_model' to access the model.\")\n",
    "else: \n",
    "    print(\"No existing YOLO model found. Please run the training cells above to train the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for repvgg and swin\n",
    "num_classes = 2\n",
    "\n",
    "def create_swin_model(num_classes):\n",
    "    swin = models.swin_t(weights=models.Swin_T_Weights.IMAGENET1K_V1)\n",
    "    # replace head for binary classification\n",
    "    swin.head = nn.Linear(swin.head.in_features, num_classes)\n",
    "    return swin\n",
    "\n",
    "\n",
    "def create_repvgg_model(num_classes):\n",
    "    # RepVGG via timm\n",
    "    model = timm.create_model(\"repvgg_b0\", pretrained=True, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "def load_existing_model(model_name, create_model_fn):\n",
    "    weight_path  = base_logs_root / f\"{model_name}_training_logs\" / \"best.pth\"\n",
    "    metrics_path = base_logs_root / f\"{model_name}_training_logs\" / \"metrics.json\"\n",
    "    if weight_path.exists():\n",
    "        print(f\"[{model_name}] Found existing weights at {weight_path}. Loading model...\")\n",
    "        model = create_model_fn(num_classes)\n",
    "        state = torch.load(weight_path, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        # Optionally load metrics as well\n",
    "        if metrics_path.exists():\n",
    "            with open(metrics_path, \"r\") as f:\n",
    "                history = json.load(f)\n",
    "            print(f\"[{model_name}] Loaded existing metrics from {metrics_path}.\")\n",
    "        else:\n",
    "            history = None\n",
    "        return model, history\n",
    "    else:\n",
    "        print(f\"[{model_name}] No existing weights found at {weight_path}. Please train the model first.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd439ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repvgg\n",
    "repvgg_model, repvgg_history = load_existing_model(\"repvgg\", create_repvgg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2021db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# swin\n",
    "swin_model, swin_history = load_existing_model(\"swin\", create_swin_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6870eb",
   "metadata": {},
   "source": [
    "# Testing with test set data \n",
    "**(Skip this section if no need to test on test sets)**)\n",
    "- 2 test sets will be used (complete test set & OOD test set)\n",
    "    - complete test set: kaggle (3 types) + custom (7 types) + ood (2 types)\n",
    "    - OOD (out-of-distribution) test set: ood (2 types) i.e., peach and tomato\n",
    "- Calculate the classification metrics on these two sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac4d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loaders & helpers\n",
    "\n",
    "test_ds = datasets.ImageFolder(\n",
    "    test_root,\n",
    "    transform=val_transform   # same normalization as val\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=32, shuffle=False,\n",
    "    num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Test classes:\", test_ds.classes)\n",
    "\n",
    "# test - ood loader \n",
    "ood_ds = datasets.ImageFolder(\n",
    "    root=ood_test_root,\n",
    "    transform=val_transform     # same preprocessing\n",
    ")\n",
    "ood_loader = DataLoader(\n",
    "    ood_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "def evaluate_on_test(\n",
    "    model,\n",
    "    loader,\n",
    "    device,\n",
    "    class_names,\n",
    "    pos_class_index=1,   # index of \"positive\" class, here: 'good' if classes=['bad','good']\n",
    "):\n",
    "    \"\"\"\n",
    "    Run model on test loader, compute confusion matrix + metrics, and plot.\n",
    "\n",
    "    Returns:\n",
    "        cm (ndarray): confusion matrix\n",
    "        metrics (dict): aggregated metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)          # logits [B, C]\n",
    "            probs = torch.softmax(outputs, dim=1)  # [B, C]\n",
    "\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_logits.append(outputs.cpu())\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    all_preds  = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "    # -------------------\n",
    "    # Confusion matrix\n",
    "    # -------------------\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    plt.title(\"Test Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # -------------------\n",
    "    # Basic metrics\n",
    "    # -------------------\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    # per-class precision/recall/F1 and macro averages\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, labels=range(len(class_names)), zero_division=0\n",
    "    )\n",
    "\n",
    "    macro_precision = precision.mean()\n",
    "    macro_recall    = recall.mean()\n",
    "    macro_f1        = f1.mean()\n",
    "\n",
    "    print(\"Classification report:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            all_labels, all_preds, target_names=class_names, zero_division=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # -------------------\n",
    "    # ROC & AUC (binary)\n",
    "    # -------------------\n",
    "    metrics = {\n",
    "        \"accuracy\": acc,\n",
    "        \"per_class_precision\": dict(zip(class_names, precision)),\n",
    "        \"per_class_recall\": dict(zip(class_names, recall)),\n",
    "        \"per_class_f1\": dict(zip(class_names, f1)),\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"macro_f1\": macro_f1,\n",
    "    }\n",
    "\n",
    "    if len(class_names) == 2:\n",
    "        # take probability of positive class (e.g., 'good' -> index 1)\n",
    "        probs = torch.softmax(torch.from_numpy(all_logits), dim=1).numpy()\n",
    "        pos_probs = probs[:, pos_class_index]\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(all_labels, pos_probs, pos_label=pos_class_index)\n",
    "        auc_value = roc_auc_score(all_labels, pos_probs)\n",
    "\n",
    "        metrics[\"roc_auc\"] = auc_value\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {auc_value:.3f})\")\n",
    "        plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC Curve (test)\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return cm, metrics\n",
    "\n",
    "\n",
    "def compute_metrics_from_matrix(cm):\n",
    "    cm = np.array(cm, dtype=float)\n",
    "    n_classes = cm.shape[0]\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        TP = cm[i, i]\n",
    "        FP = cm[:, i].sum() - TP\n",
    "        FN = cm[i, :].sum() - TP\n",
    "\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall    = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    # Print results\n",
    "    for i in range(n_classes):\n",
    "        print(f\"Class {i}:\")\n",
    "        print(f\"  Precision: {precisions[i]:.4f}\")\n",
    "        print(f\"  Recall:    {recalls[i]:.4f}\")\n",
    "        print(f\"  F1-score:  {f1s[i]:.4f}\")\n",
    "        print()\n",
    "\n",
    "    return precisions, recalls, f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25033a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo\n",
    "print(\"yolo on test set:\")\n",
    "results_yolo_test = yolo_model.val(data=\"/content/project/fruits_binary\", split=\"test\")\n",
    "\n",
    "# confusion matrix object\n",
    "cm_yolo_test = results_yolo_test.confusion_matrix.matrix\n",
    "class_names = list(yolo_model.names.values())\n",
    "class_names.append('_')\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(pd.DataFrame(\n",
    "    cm_yolo_test,\n",
    "    index=class_names,\n",
    "    columns=class_names\n",
    "))\n",
    "compute_metrics_from_matrix(cm_yolo_test)\n",
    "print('(ignore class 2)')\n",
    "\n",
    "print('')\n",
    "\n",
    "print('yolo on ood set only:')\n",
    "results_yolo_ood = yolo_model.val(data=\"/content/project/ood_test\")\n",
    "\n",
    "cm_yolo_ood = results_yolo_ood.confusion_matrix.matrix\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(pd.DataFrame(\n",
    "    cm_yolo_ood,\n",
    "    index=class_names,\n",
    "    columns=class_names\n",
    "))\n",
    "compute_metrics_from_matrix(cm_yolo_ood)\n",
    "print('(ignore class 2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0013650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repvgg\n",
    "print('repvgg on test set:')\n",
    "cm_repvgg, metrics_repvgg = evaluate_on_test(\n",
    "    model=repvgg_model,\n",
    "    loader=test_loader,\n",
    "    device=device,\n",
    "    class_names=test_ds.classes,\n",
    "    pos_class_index=1,\n",
    ")\n",
    "\n",
    "print('repvgg on ood set only:')\n",
    "cm_repvgg_ood, metrics_repvgg_ood = evaluate_on_test(\n",
    "    model=repvgg_model,\n",
    "    loader=ood_loader,\n",
    "    device=device,\n",
    "    class_names=test_ds.classes,   # ['bad', 'good']\n",
    "    pos_class_index=1,             # 'good' is index 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# swin \n",
    "print('swin on test set:')\n",
    "cm_swin, metrics_swin = evaluate_on_test(\n",
    "    model=swin_model,\n",
    "    loader=test_loader,\n",
    "    device=device,\n",
    "    class_names=test_ds.classes,   # ['bad', 'good']\n",
    "    pos_class_index=1,             # 'good' is index 1\n",
    ")\n",
    "\n",
    "print('swin on ood set only:')\n",
    "cm_swin_ood, metrics_swin_ood = evaluate_on_test(\n",
    "    model=swin_model,\n",
    "    loader=ood_loader,\n",
    "    device=device,\n",
    "    class_names=test_ds.classes,   # ['bad', 'good']\n",
    "    pos_class_index=1,             # 'good' is index 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aaea72",
   "metadata": {},
   "source": [
    "# Testing with homemade videos/photos\n",
    "- iPhone files allowed (.HEIC, .MOV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02081d61",
   "metadata": {},
   "source": [
    "### detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo detection model for bounding boxes \n",
    "detect_model = YOLO(\"yolo11n.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251de89f",
   "metadata": {},
   "source": [
    "### draw bounding boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7438e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_file(\n",
    "    file_name,\n",
    "    input_dir=\"/content/drive/MyDrive/CSC2503/fruits_vid\",\n",
    "    output_dir=\"/content/drive/MyDrive/CSC2503/fruits_vid_annotated\",\n",
    "    detect_model=detect_model,\n",
    "):\n",
    "    \"\"\"\n",
    "    Annotate a single image or video file by filename located in input_dir (default provided).\n",
    "    If `file_name` is an absolute path or contains a directory part, that path will be used directly.\n",
    "    Returns the path to the saved annotated file (str) or None on failure.\n",
    "    \"\"\"\n",
    "    # Resolve input file path: use provided absolute/path-like name as-is, otherwise join with input_dir\n",
    "    file_name = str(file_name)\n",
    "    p = Path(file_name)\n",
    "    if p.is_absolute() or p.parent != Path(\".\"):\n",
    "        file_path = p\n",
    "    else:\n",
    "        file_path = Path(input_dir) / p\n",
    "\n",
    "    if not file_path.exists():\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "    out_dir = Path(output_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    image_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".tif\", \".heic\", \".heif\", \".webp\", \".gif\"}\n",
    "    video_exts = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\", \".mpg\", \".mpeg\"}\n",
    "\n",
    "    ext = file_path.suffix.lower()\n",
    "\n",
    "    if ext in image_exts:\n",
    "        print(\"Processing image:\", file_path)\n",
    "        pil_img = Image.open(str(file_path)).convert(\"RGB\")\n",
    "        frame = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        results = detect_model(frame, verbose=False)\n",
    "\n",
    "        for r in results:\n",
    "            for box in r.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy.cpu().numpy().astype(int)[0]\n",
    "                conf = float(box.conf.cpu())\n",
    "                label = detect_model.names[int(box.cls.cpu())]\n",
    "\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "                cv2.putText(frame, f\"{label} {conf:.2f}\", (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "        out_path = out_dir / (file_path.stem + \"_det.jpg\")\n",
    "        cv2.imwrite(str(out_path), frame)\n",
    "        print(\"Saved:\", out_path)\n",
    "        return str(out_path)\n",
    "\n",
    "    elif ext in video_exts:\n",
    "        print(\"Processing video:\", file_path)\n",
    "        cap = cv2.VideoCapture(str(file_path))\n",
    "        if not cap.isOpened():\n",
    "            print(\"Cannot open:\", file_path)\n",
    "            return None\n",
    "\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "\n",
    "        out_path = out_dir / (file_path.stem + \"_det.mp4\")\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(str(out_path), fourcc, fps, (width, height))\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            results = detect_model(frame, verbose=False)\n",
    "\n",
    "            for r in results:\n",
    "                for box in r.boxes:\n",
    "                    x1, y1, x2, y2 = box.xyxy.cpu().numpy().astype(int)[0]\n",
    "                    conf = float(box.conf.cpu())\n",
    "                    label = detect_model.names[int(box.cls.cpu())]\n",
    "\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, f\"{label} {conf:.2f}\", (x1, y1 - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            out.write(frame)\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(\"Saved:\", out_path)\n",
    "        return str(out_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {ext}. Supported image ext: {sorted(image_exts)}; video ext: {sorted(video_exts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abbc8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example use case\n",
    "files_to_annotate = []\n",
    "for f in files_to_annotate: \n",
    "    annotate_file(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78120c2",
   "metadata": {},
   "source": [
    "### classify using the three models (yolo, repvgg, swin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6dd92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"not_fresh\", \"fresh\"]\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "# repvgg and swin\n",
    "@torch.no_grad()\n",
    "def predict_torch_model(model, pil_img):\n",
    "    x = inference_transform(pil_img).unsqueeze(0).to(device)\n",
    "    logits = model(x)\n",
    "    probs = torch.softmax(logits, dim=1)[0]\n",
    "    conf, idx = torch.max(probs, dim=0)\n",
    "    return class_names[idx.item()], conf.item()\n",
    "\n",
    "# yolo\n",
    "def predict_yolo_cls(yolo_model, pil_img):\n",
    "    # YOLO can take numpy or PIL; we use PIL directly\n",
    "    results = yolo_model(pil_img, verbose=False)\n",
    "    probs = results[0].probs\n",
    "    idx = int(probs.top1)\n",
    "    conf = float(probs.top1conf)\n",
    "    return class_names[idx], conf\n",
    "\n",
    "# combine the three\n",
    "def predict_all_models(pil_crop):\n",
    "    # Measure RepVGG\n",
    "    t0 = time.time()\n",
    "    rep_label, rep_conf   = predict_torch_model(repvgg_model, pil_crop)\n",
    "    t1 = time.time()\n",
    "\n",
    "    # Measure Swin\n",
    "    swin_label, swin_conf = predict_torch_model(swin_model, pil_crop)\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Measure YOLO\n",
    "    yolo_label, yolo_conf = predict_yolo_cls(yolo_model, pil_crop)\n",
    "    t3 = time.time()\n",
    "\n",
    "    times = {\n",
    "        \"RepVGG\": t1 - t0,\n",
    "        \"Swin\":   t2 - t1,\n",
    "        \"YOLO\":   t3 - t2,\n",
    "    }\n",
    "\n",
    "    preds = {\n",
    "        \"RepVGG\": (rep_label, rep_conf),\n",
    "        \"Swin\":   (swin_label, swin_conf),\n",
    "        \"YOLO\":   (yolo_label, yolo_conf),\n",
    "    }\n",
    "    return preds, times\n",
    "\n",
    "# USE THIS FUNCTION TO PREDICT ON VIDEO\n",
    "def predict_on_video(mp4_name, save_video=True, save_speed_metrics=True):\n",
    "    \"\"\"\n",
    "    input: mp4_name (str) - name of the mp4 video file in fruits_vid_annotated\n",
    "    output: optionally saves annotated video with predictions overlaid in fruits_vid_classified\n",
    "            optionally appends average per-model latencies (ms) to speed_metrics.csv in the out dir\n",
    "    \"\"\"\n",
    "    # enter your video path here\n",
    "    video_path = f\"/content/drive/MyDrive/CSC2503/fruits_vid_annotated/{mp4_name}\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Video could not be opened: {video_path}\")\n",
    "        return\n",
    "    else:\n",
    "        width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        # Ensure output directory exists (used for both video and CSV)\n",
    "        output_dir = \"/content/drive/MyDrive/CSC2503/fruits_vid_classified\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"{Path(mp4_name).stem}_classified.mp4\")\n",
    "\n",
    "        out = None\n",
    "        if save_video:\n",
    "            # Try 'avc1' (H.264) for better Drive/Web compatibility, fallback to 'mp4v'\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "            if not out.isOpened():\n",
    "                print(\"avc1 codec failed to initialize, falling back to mp4v...\")\n",
    "                fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "                out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "            if not out.isOpened():\n",
    "                print(\"Error: Could not open video writer. Disabling video saving.\")\n",
    "                out = None\n",
    "                save_video = False\n",
    "            else:\n",
    "                print(f\"Processing video... Saving to: {output_path}\")\n",
    "        else:\n",
    "            print(\"Processing video... (video saving disabled)\")\n",
    "\n",
    "        frame_counter = 0\n",
    "        # Track accumulated time per model\n",
    "        model_times = {\"RepVGG\": 0.0, \"Swin\": 0.0, \"YOLO\": 0.0}\n",
    "\n",
    "        while True:\n",
    "            ret, frame_bgr = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_counter += 1\n",
    "\n",
    "            # 1) DETECTION\n",
    "            det_results = detect_model(frame_bgr, verbose=False)\n",
    "\n",
    "            for r in det_results:\n",
    "                boxes = r.boxes.xyxy.cpu().numpy()\n",
    "                confs = r.boxes.conf.cpu().numpy()\n",
    "                clss  = r.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "                if len(boxes) == 0:\n",
    "                    continue\n",
    "\n",
    "                # pick the most confident detection for now\n",
    "                idx = confs.argmax()\n",
    "                x1, y1, x2, y2 = boxes[idx].astype(int)\n",
    "\n",
    "                # clamp to frame\n",
    "                x1 = max(0, x1); y1 = max(0, y1)\n",
    "                x2 = min(width, x2); y2 = min(height, y2)\n",
    "\n",
    "                # draw detection box\n",
    "                cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), (0,255,0), 5)\n",
    "\n",
    "                # 2) CROP + CLASSIFICATION\n",
    "                crop_bgr = frame_bgr[y1:y2, x1:x2]\n",
    "                if crop_bgr.size == 0:\n",
    "                    continue\n",
    "\n",
    "                crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)\n",
    "                pil_crop = Image.fromarray(crop_rgb)\n",
    "\n",
    "                # time the three-model prediction\n",
    "                preds, times = predict_all_models(pil_crop)\n",
    "                \n",
    "                for m, t in times.items():\n",
    "                    model_times[m] += t\n",
    "\n",
    "                # 3) OVERLAY TEXT\n",
    "                # Settings for high-res video\n",
    "                font_scale = 2.5\n",
    "                thickness_out = 8\n",
    "                thickness_in = 3\n",
    "                line_spacing = 90\n",
    "                \n",
    "                # Determine vertical position (above box if space permits, else inside)\n",
    "                total_text_height = len(preds) * line_spacing\n",
    "                y_anchor = y1 - total_text_height - 20\n",
    "                if y_anchor < 0:\n",
    "                    y_anchor = y1 + 20\n",
    "\n",
    "                for i, (name, (label, conf)) in enumerate(preds.items()):\n",
    "                    text = f\"{name}: {label} ({conf:.2f})\"\n",
    "                    y = y_anchor + (i + 1) * line_spacing\n",
    "\n",
    "                    cv2.putText(frame_bgr, text, (x1, int(y)),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0,0,0), thickness_out, cv2.LINE_AA)\n",
    "                    cv2.putText(frame_bgr, text, (x1, int(y)),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255,255,255), thickness_in, cv2.LINE_AA)\n",
    "\n",
    "            # write frame only if saving enabled\n",
    "            if save_video and out is not None:\n",
    "                out.write(frame_bgr)\n",
    "\n",
    "        cap.release()\n",
    "        if out is not None:\n",
    "            out.release()\n",
    "\n",
    "        # ---- speed metrics ----\n",
    "        print(f\"Frames processed:    {frame_counter}\")\n",
    "        print(\"Average latency per frame (classification phase only):\")\n",
    "\n",
    "        denom = max(frame_counter, 1)\n",
    "        avg_ms = {}\n",
    "        for m in [\"RepVGG\", \"Swin\", \"YOLO\"]:\n",
    "            t_total = model_times[m]\n",
    "            avg = (t_total / denom) * 1000 if denom > 0 else 0.0\n",
    "            avg_ms[m] = avg\n",
    "            print(f\"  {m}: {avg:.2f} ms\")\n",
    "\n",
    "        # Save speed metrics to CSV if requested\n",
    "        if save_speed_metrics:\n",
    "            csv_path = os.path.join(output_dir, \"speed_metrics.csv\")\n",
    "            header = [\"filename\", \"frames\", \"RepVGG_ms\", \"Swin_ms\", \"YOLO_ms\", \"timestamp\"]\n",
    "            row = {\n",
    "                \"filename\": mp4_name,\n",
    "                \"frames\": frame_counter,\n",
    "                \"RepVGG_ms\": f\"{avg_ms['RepVGG']:.2f}\",\n",
    "                \"Swin_ms\": f\"{avg_ms['Swin']:.2f}\",\n",
    "                \"YOLO_ms\": f\"{avg_ms['YOLO']:.2f}\",\n",
    "                \"timestamp\": datetime.datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "            # Use csv module to create or append\n",
    "            file_exists = os.path.exists(csv_path)\n",
    "            with open(csv_path, \"a\", newline=\"\") as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "                if not file_exists:\n",
    "                    writer.writeheader()\n",
    "                writer.writerow(row)\n",
    "            print(f\"Saved speed metrics to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd2e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example use case\n",
    "files_to_predict = [\n",
    "    'IMG_5582_det.mp4',\n",
    "    'IMG_5580_det.mp4',\n",
    "]\n",
    "for f in files_to_predict:\n",
    "    predict_on_video(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cpsc330arm)",
   "language": "python",
   "name": "cpsc330arm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
